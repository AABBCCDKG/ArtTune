# cliptonsketch

## Overview
CLIP (Contrastive Language-Image Pre-training) is a powerful model developed by OpenAI that learns visual concepts from natural language descriptions. This project focuses on fine-tuning the CLIP model on a custom [dataset](https://github.com/googlecreativelab/quickdraw-dataset) of sketches, each labeled with a specific category or prompt. The objective is to adapt the pre-trained CLIP model to better recognize and understand sketches, enhancing its performance on this specific type of visual data.
